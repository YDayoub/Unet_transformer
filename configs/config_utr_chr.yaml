model: 'vanilla-transformer' # vanilla-transformer, U-transformer
model_config:
  d_model : 128  
  dim_feedforward : 256 
  nlayers : 2 
  nhead : 4 
  dropout : 0.7 
  activation : 'gelu'
dataset_config:
  tokenizer: 'char' # spacy, basic_english, char
  dataset: 'wiki2' # wiki2, wiki103
training:
  n_epochs : 3
  batch_size:  20
  bptt: 1024
  clip_grad_norm: 0.5 
eval:
  batch_size: 8 

