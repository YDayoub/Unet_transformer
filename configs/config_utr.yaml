model: 'vanilla-transformer' # vanilla-transformer, U-transformer
model_config:
  d_model : 400  
  dim_feedforward : 1024 
  nlayers : 2
  nhead : 16
  dropout : 0.4
  emb_dropout: 0.2
  out_dropout: 0.4 
  in_dropout: 0.2
  activation : 'relu'
  tying: True
  mos: True
  save_state: False
  adv_tr: False
  weighted_connections: False
  ar_tar: False
  use_gru: False
  n_experts: 1
  gaussian: 0.2
  epsilon: 0.005
dataset_config:
  tokenizer: 'spacy' # spacy, basic_english, char, subword
  dataset: 'wiki2' # wiki2, wiki103
training:
  opt: 'linear' # linear, NoamOptimizer, sgd_platue, cyclic_linear,  RAdam_platue
  n_epochs : 1
  batch_size:  20
  bptt: 256
  clip_grad_norm: 0.5
  use_aux: False
  adaptive_dropout: False
  save_model: True
  logging: False
  use_var_len: False
  weight_aux: 0.01
  weight_decay: 1e-5
  alpha: 0
  beta: 0
  partial_shuffling: False
  use_average: False
eval:
  batch_size: 1 
test:
  batch_size: 1



