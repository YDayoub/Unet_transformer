model: 'U-transformer' # vanilla-transformer, U-transformer
model_config:
  d_model : 512  
  dim_feedforward : 1024 
  nlayers : 2
  nhead : 4 
  dropout : 0.2 
  activation : 'relu'
dataset_config:
  tokenizer: 'spacy' # spacy, basic_english, char
  dataset: 'wiki2' # wiki2, wiki103
training:
  opt: 'linear' # linear, Noam
  n_epochs : 30
  batch_size:  20
  bptt: 128
  clip_grad_norm: 0.5
  use_aux: False
  adaptive_dropout: True
  save_model: False
  logging: False
  use_var_len: True
  weight_aux: 0.1
eval:
  batch_size: 8 



